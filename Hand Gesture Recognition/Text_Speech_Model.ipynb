{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-time Hand Gesture Recognition with Speech,Text Feedback using Custom Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 136ms/step\n",
      "Recognized Gesture: J\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Recognized Gesture: V\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: J\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: 2\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Recognized Gesture: V\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: 8\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: V\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Recognized Gesture: 4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Recognized Gesture: K\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Recognized Gesture: 5\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: 3\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: K\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: 3\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: 5\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Recognized Gesture: 2\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Recognized Gesture: 5\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: 4\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Recognized Gesture: 5\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: 4\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: 3\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: 4\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: 3\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Recognized Gesture: 4\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: U\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: 1\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: 6\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: 1\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: 4\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: U\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: 4\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: 3\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: 4\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: 3\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: 6\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: L\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: R\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Recognized Gesture: U\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: 6\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: R\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: 6\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: U\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: 6\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: Z\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: W\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Recognized Gesture: H\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: 9\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: I\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Recognized Gesture: V\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gtts import gTTS\n",
    "import pygame\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = tf.keras.models.load_model('/Users/sujaykaushik/Documents/Msc DataScience/Project Dissertation/Extra  files/OwnModel_15Oct_A_Z_1_9.h5')\n",
    "\n",
    "# Define the coordinates of the region of interest (ROI) box\n",
    "roi_x, roi_y, roi_width, roi_height = 100, 100, 250, 250\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize pygame for audio playback\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Initialize previous gesture label\n",
    "prev_gesture_label = None\n",
    "\n",
    "# Create a text file to save the recognized output\n",
    "text_file = open(\"recognized_output_2.txt\", \"w\")\n",
    "\n",
    "# Create a separate window for displaying the hand region\n",
    "cv2.namedWindow('Hand Region', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('Hand Region', roi_width, roi_height)\n",
    "\n",
    "while True:\n",
    "    # Capture a frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply binary thresholding to convert to black and white\n",
    "    _, bw_image = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Invert the colors (hand appears white, background appears black)\n",
    "    inverted_bw_image = cv2.bitwise_not(bw_image)\n",
    "\n",
    "    # Extract the hand region within the ROI\n",
    "    hand_roi = inverted_bw_image[roi_y:roi_y + roi_height, roi_x:roi_x + roi_width]\n",
    "\n",
    "    # Convert hand_roi to a 3D array for compatibility\n",
    "    hand_roi_colored = cv2.cvtColor(hand_roi, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Create an empty black image for the bounding box part\n",
    "    bounding_box_image = np.zeros_like(frame)\n",
    "\n",
    "    # Overlay the hand_roi on the bounding_box_image\n",
    "    bounding_box_image[roi_y:roi_y + roi_height, roi_x:roi_x + roi_width] = hand_roi_colored\n",
    "\n",
    "    # Display the bounding box part as binary black and white\n",
    "    cv2.imshow('Bounding Box Binary Image', bounding_box_image)\n",
    "\n",
    "    # Display the hand region in a separate window\n",
    "    cv2.imshow('Hand Region', hand_roi_colored)\n",
    "\n",
    "    # Preprocess the hand_roi image for prediction\n",
    "    resized_hand_roi = cv2.resize(hand_roi, (32, 32))  # Resize to (32, 32)\n",
    "    rgb_hand_roi = cv2.cvtColor(resized_hand_roi, cv2.COLOR_GRAY2RGB)  # Convert to RGB\n",
    "\n",
    "    # Normalize the RGB values to the range [0, 1]\n",
    "    normalized_hand_roi = rgb_hand_roi / 255.0\n",
    "\n",
    "    # Perform gesture prediction using the pre-trained model\n",
    "    predictions = model.predict(np.expand_dims(normalized_hand_roi, axis=0))\n",
    "    predicted_gesture_index = np.argmax(predictions)\n",
    "\n",
    "    # Get the predicted gesture label\n",
    "    gestures = ['1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "    predicted_gesture_label = gestures[predicted_gesture_index]\n",
    "\n",
    "    if predicted_gesture_label != prev_gesture_label:\n",
    "        # Convert predicted gesture label to speech\n",
    "        tts = gTTS(text=predicted_gesture_label, lang='en')\n",
    "        tts.save('predicted_audio.mp3')\n",
    "\n",
    "        # Play the saved audio file using pygame\n",
    "        pygame.mixer.music.load('predicted_audio.mp3')\n",
    "        pygame.mixer.music.play()\n",
    "\n",
    "        # Write the predicted gesture to the text file\n",
    "        text_file.write(predicted_gesture_label + '\\n')\n",
    "        print(f\"Recognized Gesture: {predicted_gesture_label}\")\n",
    "\n",
    "    # Update the previous gesture label\n",
    "    prev_gesture_label = predicted_gesture_label\n",
    "\n",
    "    # Draw the ROI box on the frame\n",
    "    cv2.rectangle(frame, (roi_x, roi_y), (roi_x + roi_width, roi_y + roi_height), (0, 255, 0), 2)\n",
    "\n",
    "    # Display the predicted gesture on the frame\n",
    "    cv2.putText(frame, predicted_gesture_label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame with ROI and predicted gesture\n",
    "    cv2.imshow('Gesture Prediction', frame)\n",
    "\n",
    "    # Press 'q' to quit the capture loop\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close the windows\n",
    "text_file.close()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-time Hand Gesture Recognition with Speech,Text Feedback using VGG-16 Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 181ms/step\n",
      "Recognized Gesture: V\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Recognized Gesture: C\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Recognized Gesture: V\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Recognized Gesture: J\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Recognized Gesture: V\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Recognized Gesture: 1\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Recognized Gesture: V\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Recognized Gesture: 2\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Recognized Gesture: V\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gtts import gTTS\n",
    "import pygame\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = tf.keras.models.load_model('/Users/sujaykaushik/Documents/Msc DataScience/Project Dissertation/Extra  files/VGG16_Dec17.h5')\n",
    "\n",
    "# Define the coordinates of the region of interest (ROI) box\n",
    "roi_x, roi_y, roi_width, roi_height = 100, 100, 250, 250\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize pygame for audio playback\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Initialize previous gesture label\n",
    "prev_gesture_label = None\n",
    "\n",
    "# Create a text file to save the recognized output\n",
    "text_file = open(\"recognized_output_2.txt\", \"w\")\n",
    "\n",
    "# Create a separate window for displaying the hand region\n",
    "cv2.namedWindow('Hand Region', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('Hand Region', roi_width, roi_height)\n",
    "\n",
    "while True:\n",
    "    # Capture a frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply binary thresholding to convert to black and white\n",
    "    _, bw_image = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Invert the colors (hand appears white, background appears black)\n",
    "    inverted_bw_image = cv2.bitwise_not(bw_image)\n",
    "\n",
    "    # Extract the hand region within the ROI\n",
    "    hand_roi = inverted_bw_image[roi_y:roi_y + roi_height, roi_x:roi_x + roi_width]\n",
    "\n",
    "    # Convert hand_roi to a 3D array for compatibility\n",
    "    hand_roi_colored = cv2.cvtColor(hand_roi, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Create an empty black image for the bounding box part\n",
    "    bounding_box_image = np.zeros_like(frame)\n",
    "\n",
    "    # Overlay the hand_roi on the bounding_box_image\n",
    "    bounding_box_image[roi_y:roi_y + roi_height, roi_x:roi_x + roi_width] = hand_roi_colored\n",
    "\n",
    "    # Display the bounding box part as binary black and white\n",
    "    cv2.imshow('Bounding Box Binary Image', bounding_box_image)\n",
    "\n",
    "    # Display the hand region in a separate window\n",
    "    cv2.imshow('Hand Region', hand_roi_colored)\n",
    "\n",
    "    # Preprocess the hand_roi image for prediction\n",
    "    resized_hand_roi = cv2.resize(hand_roi, (32, 32))  # Resize to (32, 32)\n",
    "    rgb_hand_roi = cv2.cvtColor(resized_hand_roi, cv2.COLOR_GRAY2RGB)  # Convert to RGB\n",
    "\n",
    "    # Normalize the RGB values to the range [0, 1]\n",
    "    normalized_hand_roi = rgb_hand_roi / 255.0\n",
    "\n",
    "    # Perform gesture prediction using the pre-trained model\n",
    "    predictions = model.predict(np.expand_dims(normalized_hand_roi, axis=0))\n",
    "    predicted_gesture_index = np.argmax(predictions)\n",
    "\n",
    "    # Get the predicted gesture label\n",
    "    gestures = ['1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "    predicted_gesture_label = gestures[predicted_gesture_index]\n",
    "\n",
    "    if predicted_gesture_label != prev_gesture_label:\n",
    "        # Convert predicted gesture label to speech\n",
    "        tts = gTTS(text=predicted_gesture_label, lang='en')\n",
    "        tts.save('predicted_audio.mp3')\n",
    "\n",
    "        # Play the saved audio file using pygame\n",
    "        pygame.mixer.music.load('predicted_audio.mp3')\n",
    "        pygame.mixer.music.play()\n",
    "\n",
    "        # Write the predicted gesture to the text file\n",
    "        text_file.write(predicted_gesture_label + '\\n')\n",
    "        print(f\"Recognized Gesture: {predicted_gesture_label}\")\n",
    "\n",
    "    # Update the previous gesture label\n",
    "    prev_gesture_label = predicted_gesture_label\n",
    "\n",
    "    # Draw the ROI box on the frame\n",
    "    cv2.rectangle(frame, (roi_x, roi_y), (roi_x + roi_width, roi_y + roi_height), (0, 255, 0), 2)\n",
    "\n",
    "    # Display the predicted gesture on the frame\n",
    "    cv2.putText(frame, predicted_gesture_label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame with ROI and predicted gesture\n",
    "    cv2.imshow('Gesture Prediction', frame)\n",
    "\n",
    "    # Press 'q' to quit the capture loop\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close the windows\n",
    "text_file.close()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
